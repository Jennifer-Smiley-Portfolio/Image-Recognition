{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flower Image Recognition Program\n",
    "\n",
    "#### Using ImageAI\n",
    "##### By: Jennifer Smiley"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing  the ImageAI pack that has dependancies on Tensorflow, Karas, OpenCSV, and Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageai.Prediction.Custom import ModelTraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Line one below defines what type of training, the second line defines the network type, and the third line sets the path\n",
    "to the images in my folder. \n",
    "\n",
    "The folder is named flower and inside I have it separated into train and test folders.\n",
    "Both folders have 5 folders inside with the names of the flowers as instructed by the ImageAI documentation. Then the photos\n",
    "for each flower are inside their correponding folder. About 80% of the data is in the training folder and the remaining \n",
    "20% is in the testing folder. \n",
    "\n",
    "The last line, in theory, should start the training process. \n",
    "\n",
    "num_objects: the number of object types in the folder.\n",
    "    \n",
    "num_experiments: the number of times the network will train over all the training images, which is also called epochs\n",
    "    \n",
    "enhance_data = True: Is used to state if you want the network to produce modified copies of the \n",
    "    training images for better performance.\n",
    "\n",
    "batch_size: Is to state the number of images the network will process at ones. The images are processed in batches until they are exhausted per each experiment performed.\n",
    "\n",
    "show_network_summary = True: If you want to see the output of the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunatly, I am having an error with their code on line 4. Which is stopping the code from creating a .json file which\n",
    "will be needed in the second part of the code. \n",
    "FIXED: See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer = ModelTraining()\n",
    "model_trainer.setModelTypeAsResNet()\n",
    "model_trainer.setDataDirectory(\"C:/Users/athen/Documents/CDS 490/flower\")\n",
    "#model_trainer.trainModel(num_objects=1, num_experiments=100, enhance_data=True, batch_size=32, show_network_summary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\athen\\Documents\\Anaconda\\Anaconda_2\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 112, 112, 64) 9472        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 112, 112, 64) 256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 112, 112, 64) 0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 55, 55, 64)   0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 55, 55, 64)   4160        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 55, 55, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 55, 55, 64)   0           batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 55, 55, 64)   36928       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 55, 55, 64)   256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 55, 55, 64)   0           batch_normalization_v1_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 55, 55, 256)  16640       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 55, 55, 256)  16640       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 55, 55, 256)  1024        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 55, 55, 256)  1024        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 55, 55, 256)  0           batch_normalization_v1_4[0][0]   \n",
      "                                                                 batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 55, 55, 256)  0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 55, 55, 64)   16448       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 55, 55, 64)   256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 55, 55, 64)   0           batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 55, 55, 64)   36928       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 55, 55, 64)   256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 55, 55, 64)   0           batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 55, 55, 256)  16640       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 55, 55, 256)  1024        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 55, 55, 256)  0           batch_normalization_v1_7[0][0]   \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 55, 55, 256)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 55, 55, 64)   16448       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 55, 55, 64)   256         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 55, 55, 64)   0           batch_normalization_v1_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 55, 55, 64)   36928       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_9 (Batch (None, 55, 55, 64)   256         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 55, 55, 64)   0           batch_normalization_v1_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 55, 55, 256)  16640       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_10 (Batc (None, 55, 55, 256)  1024        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 55, 55, 256)  0           batch_normalization_v1_10[0][0]  \n",
      "                                                                 activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 55, 55, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 28, 28, 128)  32896       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_12 (Batc (None, 28, 28, 128)  512         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 28, 28, 128)  0           batch_normalization_v1_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 28, 28, 128)  147584      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_13 (Batc (None, 28, 28, 128)  512         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 28, 28, 128)  0           batch_normalization_v1_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 28, 28, 512)  66048       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 28, 28, 512)  131584      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_14 (Batc (None, 28, 28, 512)  2048        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_11 (Batc (None, 28, 28, 512)  2048        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 28, 28, 512)  0           batch_normalization_v1_14[0][0]  \n",
      "                                                                 batch_normalization_v1_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 28, 28, 512)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 28, 28, 128)  65664       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_15 (Batc (None, 28, 28, 128)  512         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 28, 28, 128)  0           batch_normalization_v1_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 28, 28, 128)  147584      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_16 (Batc (None, 28, 28, 128)  512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 28, 28, 128)  0           batch_normalization_v1_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 28, 28, 512)  66048       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_17 (Batc (None, 28, 28, 512)  2048        conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 28, 28, 512)  0           batch_normalization_v1_17[0][0]  \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 28, 28, 512)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 28, 28, 128)  65664       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_18 (Batc (None, 28, 28, 128)  512         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 28, 28, 128)  0           batch_normalization_v1_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 28, 28, 128)  147584      activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_19 (Batc (None, 28, 28, 128)  512         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 28, 28, 128)  0           batch_normalization_v1_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 28, 28, 512)  66048       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_20 (Batc (None, 28, 28, 512)  2048        conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 28, 28, 512)  0           batch_normalization_v1_20[0][0]  \n",
      "                                                                 activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 28, 28, 512)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 28, 28, 128)  65664       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_21 (Batc (None, 28, 28, 128)  512         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 28, 28, 128)  0           batch_normalization_v1_21[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 28, 28, 128)  147584      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_22 (Batc (None, 28, 28, 128)  512         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 28, 28, 128)  0           batch_normalization_v1_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 28, 28, 512)  66048       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_23 (Batc (None, 28, 28, 512)  2048        conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 28, 28, 512)  0           batch_normalization_v1_23[0][0]  \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 28, 28, 512)  0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 14, 14, 256)  131328      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_25 (Batc (None, 14, 14, 256)  1024        conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 14, 14, 256)  0           batch_normalization_v1_25[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 14, 14, 256)  590080      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_26 (Batc (None, 14, 14, 256)  1024        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 14, 14, 256)  0           batch_normalization_v1_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 14, 14, 1024) 263168      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 14, 14, 1024) 525312      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_27 (Batc (None, 14, 14, 1024) 4096        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_24 (Batc (None, 14, 14, 1024) 4096        conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 14, 14, 1024) 0           batch_normalization_v1_27[0][0]  \n",
      "                                                                 batch_normalization_v1_24[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 14, 14, 1024) 0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 14, 14, 256)  262400      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_28 (Batc (None, 14, 14, 256)  1024        conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 14, 14, 256)  0           batch_normalization_v1_28[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 14, 14, 256)  590080      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_29 (Batc (None, 14, 14, 256)  1024        conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 14, 14, 256)  0           batch_normalization_v1_29[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 14, 14, 1024) 263168      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_30 (Batc (None, 14, 14, 1024) 4096        conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 14, 14, 1024) 0           batch_normalization_v1_30[0][0]  \n",
      "                                                                 activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 14, 14, 1024) 0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 14, 14, 256)  262400      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_31 (Batc (None, 14, 14, 256)  1024        conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 14, 14, 256)  0           batch_normalization_v1_31[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 14, 14, 256)  590080      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_32 (Batc (None, 14, 14, 256)  1024        conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 14, 14, 256)  0           batch_normalization_v1_32[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 14, 14, 1024) 263168      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 14, 14, 1024) 4096        conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 14, 14, 1024) 0           batch_normalization_v1_33[0][0]  \n",
      "                                                                 activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 14, 14, 1024) 0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 14, 14, 256)  262400      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 14, 14, 256)  1024        conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 14, 14, 256)  0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 14, 14, 256)  590080      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 14, 14, 256)  1024        conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 14, 14, 256)  0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 14, 14, 1024) 263168      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 14, 14, 1024) 4096        conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 14, 14, 1024) 0           batch_normalization_v1_36[0][0]  \n",
      "                                                                 activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 14, 14, 1024) 0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 14, 14, 256)  262400      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 14, 14, 256)  1024        conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 14, 14, 256)  0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 14, 14, 256)  590080      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 14, 14, 256)  1024        conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 14, 14, 256)  0           batch_normalization_v1_38[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 14, 14, 1024) 263168      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 14, 14, 1024) 4096        conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 14, 14, 1024) 0           batch_normalization_v1_39[0][0]  \n",
      "                                                                 activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 14, 14, 1024) 0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 14, 14, 256)  262400      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 14, 14, 256)  1024        conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 14, 14, 256)  0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 14, 14, 256)  590080      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 14, 14, 256)  1024        conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 14, 14, 256)  0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 14, 14, 1024) 263168      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 14, 14, 1024) 4096        conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 14, 14, 1024) 0           batch_normalization_v1_42[0][0]  \n",
      "                                                                 activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 14, 14, 1024) 0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 7, 7, 512)    524800      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 7, 7, 512)    2048        conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 7, 7, 512)    0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 7, 7, 512)    2359808     activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 7, 7, 512)    2048        conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 7, 7, 512)    0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 7, 7, 2048)   1050624     activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 7, 7, 2048)   2099200     activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 7, 7, 2048)   8192        conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 7, 7, 2048)   8192        conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 7, 7, 2048)   0           batch_normalization_v1_46[0][0]  \n",
      "                                                                 batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 7, 7, 2048)   0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 7, 7, 512)    1049088     activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 7, 7, 512)    2048        conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 7, 7, 512)    0           batch_normalization_v1_47[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 7, 7, 512)    2359808     activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 7, 7, 512)    2048        conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 7, 7, 512)    0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 7, 7, 2048)   1050624     activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 7, 7, 2048)   8192        conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 7, 7, 2048)   0           batch_normalization_v1_49[0][0]  \n",
      "                                                                 activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 7, 7, 2048)   0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 7, 7, 512)    1049088     activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 7, 7, 512)    2048        conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 7, 7, 512)    0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 7, 7, 512)    2359808     activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 7, 7, 512)    2048        conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 7, 7, 512)    0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 7, 7, 2048)   1050624     activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 7, 7, 2048)   8192        conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 7, 7, 2048)   0           batch_normalization_v1_52[0][0]  \n",
      "                                                                 activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 7, 7, 2048)   0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_avg_pooling (GlobalAvera (None, 2048)         0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 5)            10245       global_avg_pooling[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 5)            0           dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 23,597,957\n",
      "Trainable params: 23,544,837\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n",
      "Using Enhanced Data Generation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3456 images belonging to 5 classes.\n",
      "Found 867 images belonging to 5 classes.\n",
      "JSON Mapping for the model classes saved to  C:/Users/athen/Documents/CDS 490/flower\\json\\model_class.json\n",
      "Number of experiments (Epochs) :  100\n",
      "WARNING:tensorflow:From C:\\Users\\athen\\Documents\\Anaconda\\Anaconda_2\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "28/28 [==============================] - 133s 5s/step - loss: 1.6251 - acc: 0.2191\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.21915, saving model to C:/Users/athen/Documents/CDS 490/flower\\models\\model_ex-001_acc-0.219146.h5\n",
      "108/108 [==============================] - 7459s 69s/step - loss: 2.0323 - acc: 0.3987 - val_loss: 1.6251 - val_acc: 0.2191\n",
      "Epoch 2/100\n",
      "28/28 [==============================] - 130s 5s/step - loss: 1.6928 - acc: 0.2907\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.21915 to 0.29066, saving model to C:/Users/athen/Documents/CDS 490/flower\\models\\model_ex-002_acc-0.290657.h5\n",
      "108/108 [==============================] - 7674s 71s/step - loss: 1.4161 - acc: 0.5133 - val_loss: 1.6928 - val_acc: 0.2907\n",
      "Epoch 3/100\n",
      "28/28 [==============================] - 137s 5s/step - loss: 13.4644 - acc: 0.1569\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.29066\n",
      "108/108 [==============================] - 7548s 70s/step - loss: 1.5597 - acc: 0.5084 - val_loss: 13.4644 - val_acc: 0.1569\n",
      "Epoch 4/100\n",
      "28/28 [==============================] - 131s 5s/step - loss: 6.2817 - acc: 0.3068\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.29066 to 0.30681, saving model to C:/Users/athen/Documents/CDS 490/flower\\models\\model_ex-004_acc-0.306805.h5\n",
      "108/108 [==============================] - 7506s 69s/step - loss: 1.5564 - acc: 0.4800 - val_loss: 6.2817 - val_acc: 0.3068\n",
      "Epoch 5/100\n",
      "28/28 [==============================] - 129s 5s/step - loss: 1.5204 - acc: 0.3737\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.30681 to 0.37370, saving model to C:/Users/athen/Documents/CDS 490/flower\\models\\model_ex-005_acc-0.373702.h5\n",
      "108/108 [==============================] - 7427s 69s/step - loss: 1.1807 - acc: 0.5495 - val_loss: 1.5204 - val_acc: 0.3737\n",
      "Epoch 6/100\n",
      "28/28 [==============================] - 132s 5s/step - loss: 1.5067 - acc: 0.4141\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.37370 to 0.41407, saving model to C:/Users/athen/Documents/CDS 490/flower\\models\\model_ex-006_acc-0.414072.h5\n",
      "108/108 [==============================] - 7527s 70s/step - loss: 1.1070 - acc: 0.5903 - val_loss: 1.5067 - val_acc: 0.4141\n",
      "Epoch 7/100\n",
      "28/28 [==============================] - 130s 5s/step - loss: 2.3658 - acc: 0.4245\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.41407 to 0.42445, saving model to C:/Users/athen/Documents/CDS 490/flower\\models\\model_ex-007_acc-0.424452.h5\n",
      "108/108 [==============================] - 7366s 68s/step - loss: 1.0241 - acc: 0.6140 - val_loss: 2.3658 - val_acc: 0.4245\n",
      "Epoch 8/100\n",
      "28/28 [==============================] - 132s 5s/step - loss: 3.8693 - acc: 0.1926\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.42445\n",
      "108/108 [==============================] - 7384s 68s/step - loss: 1.0422 - acc: 0.6108 - val_loss: 3.8693 - val_acc: 0.1926\n",
      "Epoch 9/100\n",
      "28/28 [==============================] - 130s 5s/step - loss: 2.1840 - acc: 0.4729\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.42445 to 0.47290, saving model to C:/Users/athen/Documents/CDS 490/flower\\models\\model_ex-009_acc-0.472895.h5\n",
      "108/108 [==============================] - 7408s 69s/step - loss: 1.0096 - acc: 0.6149 - val_loss: 2.1840 - val_acc: 0.4729\n",
      "Epoch 10/100\n",
      "28/28 [==============================] - 134s 5s/step - loss: 2.8087 - acc: 0.3737\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.47290\n",
      "108/108 [==============================] - 7446s 69s/step - loss: 0.9448 - acc: 0.6499 - val_loss: 2.8087 - val_acc: 0.3737\n",
      "Epoch 11/100\n",
      "28/28 [==============================] - 130s 5s/step - loss: 2.0658 - acc: 0.5040\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.47290 to 0.50404, saving model to C:/Users/athen/Documents/CDS 490/flower\\models\\model_ex-011_acc-0.504037.h5\n",
      "108/108 [==============================] - 7529s 70s/step - loss: 0.9064 - acc: 0.6617 - val_loss: 2.0658 - val_acc: 0.5040\n",
      "Epoch 12/100\n",
      "28/28 [==============================] - 129s 5s/step - loss: 1.2409 - acc: 0.5525\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.50404 to 0.55248, saving model to C:/Users/athen/Documents/CDS 490/flower\\models\\model_ex-012_acc-0.552480.h5\n",
      "108/108 [==============================] - 7422s 69s/step - loss: 0.8883 - acc: 0.6719 - val_loss: 1.2409 - val_acc: 0.5525\n",
      "Epoch 13/100\n",
      "28/28 [==============================] - 130s 5s/step - loss: 1.1503 - acc: 0.5813\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.55248 to 0.58131, saving model to C:/Users/athen/Documents/CDS 490/flower\\models\\model_ex-013_acc-0.581315.h5\n",
      "108/108 [==============================] - 7464s 69s/step - loss: 0.8440 - acc: 0.6803 - val_loss: 1.1503 - val_acc: 0.5813\n",
      "Epoch 14/100\n",
      "28/28 [==============================] - 130s 5s/step - loss: 1.1600 - acc: 0.5698\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.58131\n",
      "108/108 [==============================] - 23776s 220s/step - loss: 0.8320 - acc: 0.6907 - val_loss: 1.1600 - val_acc: 0.5698\n",
      "Epoch 15/100\n",
      "28/28 [==============================] - 136s 5s/step - loss: 1.6780 - acc: 0.5998\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.58131 to 0.59977, saving model to C:/Users/athen/Documents/CDS 490/flower\\models\\model_ex-015_acc-0.599769.h5\n",
      "108/108 [==============================] - 7570s 70s/step - loss: 0.7893 - acc: 0.7130 - val_loss: 1.6780 - val_acc: 0.5998\n",
      "Epoch 16/100\n",
      "28/28 [==============================] - 130s 5s/step - loss: 1.0835 - acc: 0.5928\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.59977\n",
      "108/108 [==============================] - 7431s 69s/step - loss: 0.8039 - acc: 0.7046 - val_loss: 1.0835 - val_acc: 0.5928\n",
      "Epoch 17/100\n",
      "28/28 [==============================] - 130s 5s/step - loss: 1.7211 - acc: 0.5871\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.59977\n",
      "108/108 [==============================] - 7425s 69s/step - loss: 0.7553 - acc: 0.7248 - val_loss: 1.7211 - val_acc: 0.5871\n",
      "Epoch 18/100\n",
      "28/28 [==============================] - 130s 5s/step - loss: 1.6562 - acc: 0.5594\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.59977\n",
      "108/108 [==============================] - 7444s 69s/step - loss: 0.7274 - acc: 0.7312 - val_loss: 1.6562 - val_acc: 0.5594\n",
      "Epoch 19/100\n",
      "28/28 [==============================] - 133s 5s/step - loss: 1.8792 - acc: 0.4913\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.59977\n",
      "108/108 [==============================] - 7420s 69s/step - loss: 0.7472 - acc: 0.7248 - val_loss: 1.8792 - val_acc: 0.4913\n",
      "Epoch 20/100\n",
      "28/28 [==============================] - 130s 5s/step - loss: 1.2132 - acc: 0.5490\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.59977\n",
      "108/108 [==============================] - 7421s 69s/step - loss: 0.7319 - acc: 0.7234 - val_loss: 1.2132 - val_acc: 0.5490\n",
      "Epoch 21/100\n",
      "28/28 [==============================] - 174s 6s/step - loss: 1.9140 - acc: 0.5271\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.59977\n",
      "108/108 [==============================] - 9858s 91s/step - loss: 0.7140 - acc: 0.7387 - val_loss: 1.9140 - val_acc: 0.5271\n",
      "Epoch 22/100\n",
      "28/28 [==============================] - 174s 6s/step - loss: 1.0584 - acc: 0.6413\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.59977 to 0.64129, saving model to C:/Users/athen/Documents/CDS 490/flower\\models\\model_ex-022_acc-0.641292.h5\n",
      "108/108 [==============================] - 12708s 118s/step - loss: 0.6590 - acc: 0.7584 - val_loss: 1.0584 - val_acc: 0.6413\n",
      "Epoch 23/100\n",
      "28/28 [==============================] - 165s 6s/step - loss: 0.9903 - acc: 0.6344\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.64129\n",
      "108/108 [==============================] - 9834s 91s/step - loss: 0.6757 - acc: 0.7471 - val_loss: 0.9903 - val_acc: 0.6344\n",
      "Epoch 24/100\n",
      "28/28 [==============================] - 166s 6s/step - loss: 1.2106 - acc: 0.5963\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.64129\n",
      "108/108 [==============================] - 9846s 91s/step - loss: 0.6507 - acc: 0.7648 - val_loss: 1.2106 - val_acc: 0.5963\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 167s 6s/step - loss: 1.2160 - acc: 0.6136\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.64129\n",
      "108/108 [==============================] - 9848s 91s/step - loss: 0.6674 - acc: 0.7662 - val_loss: 1.2160 - val_acc: 0.6136\n",
      "Epoch 26/100\n",
      "28/28 [==============================] - 169s 6s/step - loss: 1.0441 - acc: 0.6563\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.64129 to 0.65629, saving model to C:/Users/athen/Documents/CDS 490/flower\\models\\model_ex-026_acc-0.656286.h5\n",
      "108/108 [==============================] - 9812s 91s/step - loss: 0.6080 - acc: 0.7749 - val_loss: 1.0441 - val_acc: 0.6563\n",
      "Epoch 27/100\n",
      "28/28 [==============================] - 169s 6s/step - loss: 2.4607 - acc: 0.5248\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.65629\n",
      "108/108 [==============================] - 9812s 91s/step - loss: 0.6813 - acc: 0.7543 - val_loss: 2.4607 - val_acc: 0.5248\n",
      "Epoch 28/100\n",
      "28/28 [==============================] - 166s 6s/step - loss: 1.6039 - acc: 0.5525\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.65629\n",
      "108/108 [==============================] - 9793s 91s/step - loss: 0.6506 - acc: 0.7572 - val_loss: 1.6039 - val_acc: 0.5525\n",
      "Epoch 29/100\n",
      "28/28 [==============================] - 169s 6s/step - loss: 0.9569 - acc: 0.6632\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.65629 to 0.66321, saving model to C:/Users/athen/Documents/CDS 490/flower\\models\\model_ex-029_acc-0.663206.h5\n",
      "108/108 [==============================] - 9767s 90s/step - loss: 0.6274 - acc: 0.7740 - val_loss: 0.9569 - val_acc: 0.6632\n",
      "Epoch 30/100\n",
      "28/28 [==============================] - 164s 6s/step - loss: 1.0068 - acc: 0.6574\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.66321\n",
      "108/108 [==============================] - 9951s 92s/step - loss: 0.6176 - acc: 0.7679 - val_loss: 1.0068 - val_acc: 0.6574\n",
      "Epoch 31/100\n",
      "28/28 [==============================] - 166s 6s/step - loss: 1.3402 - acc: 0.5917\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.66321\n",
      "108/108 [==============================] - 11319s 105s/step - loss: 0.5656 - acc: 0.7960 - val_loss: 1.3402 - val_acc: 0.5917\n",
      "Epoch 32/100\n",
      "28/28 [==============================] - 169s 6s/step - loss: 1.8495 - acc: 0.4614\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.66321\n",
      "108/108 [==============================] - 9834s 91s/step - loss: 0.5756 - acc: 0.7954 - val_loss: 1.8495 - val_acc: 0.4614\n",
      "Epoch 33/100\n",
      "28/28 [==============================] - 165s 6s/step - loss: 1.8501 - acc: 0.4879\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.66321\n",
      "108/108 [==============================] - 10098s 93s/step - loss: 0.5556 - acc: 0.7914 - val_loss: 1.8501 - val_acc: 0.4879\n",
      "Epoch 34/100\n",
      "28/28 [==============================] - 164s 6s/step - loss: 0.9591 - acc: 0.6909\n",
      "\n",
      "Epoch 00034: val_acc improved from 0.66321 to 0.69089, saving model to C:/Users/athen/Documents/CDS 490/flower\\models\\model_ex-034_acc-0.690888.h5\n",
      "108/108 [==============================] - 9763s 90s/step - loss: 0.5416 - acc: 0.7986 - val_loss: 0.9591 - val_acc: 0.6909\n",
      "Epoch 35/100\n",
      "28/28 [==============================] - 165s 6s/step - loss: 1.3671 - acc: 0.6344\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.69089\n",
      "108/108 [==============================] - 9726s 90s/step - loss: 0.5062 - acc: 0.8180 - val_loss: 1.3671 - val_acc: 0.6344\n",
      "Epoch 36/100\n",
      "28/28 [==============================] - 166s 6s/step - loss: 1.5009 - acc: 0.5767\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.69089\n",
      "108/108 [==============================] - 9718s 90s/step - loss: 0.5229 - acc: 0.8096 - val_loss: 1.5009 - val_acc: 0.5767\n",
      "Epoch 37/100\n",
      "28/28 [==============================] - 221s 8s/step - loss: 1.4593 - acc: 0.5779\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.69089\n",
      "108/108 [==============================] - 9850s 91s/step - loss: 0.5094 - acc: 0.8099 - val_loss: 1.4593 - val_acc: 0.5779\n",
      "Epoch 38/100\n",
      "28/28 [==============================] - 163s 6s/step - loss: 1.1235 - acc: 0.6424\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.69089\n",
      "108/108 [==============================] - 12225s 113s/step - loss: 0.4897 - acc: 0.8186 - val_loss: 1.1235 - val_acc: 0.6424\n",
      "Epoch 39/100\n",
      "28/28 [==============================] - 134s 5s/step - loss: 1.6557 - acc: 0.5502\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.69089\n",
      "108/108 [==============================] - 9268s 86s/step - loss: 0.4751 - acc: 0.8241 - val_loss: 1.6557 - val_acc: 0.5502\n",
      "Epoch 40/100\n",
      "28/28 [==============================] - 171s 6s/step - loss: 1.1436 - acc: 0.6182\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.69089\n",
      "108/108 [==============================] - 12113s 112s/step - loss: 0.4612 - acc: 0.8383 - val_loss: 1.1436 - val_acc: 0.6182\n",
      "Epoch 41/100\n",
      "28/28 [==============================] - 172s 6s/step - loss: 1.4180 - acc: 0.6228\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.69089\n",
      "108/108 [==============================] - 9901s 92s/step - loss: 0.4625 - acc: 0.8304 - val_loss: 1.4180 - val_acc: 0.6228\n",
      "Epoch 42/100\n",
      "28/28 [==============================] - 164s 6s/step - loss: 0.8647 - acc: 0.7140\n",
      "\n",
      "Epoch 00042: val_acc improved from 0.69089 to 0.71396, saving model to C:/Users/athen/Documents/CDS 490/flower\\models\\model_ex-042_acc-0.713956.h5\n",
      "108/108 [==============================] - 10347s 96s/step - loss: 0.3642 - acc: 0.8660 - val_loss: 0.8647 - val_acc: 0.7140\n",
      "Epoch 43/100\n",
      "28/28 [==============================] - 164s 6s/step - loss: 0.9121 - acc: 0.7220\n",
      "\n",
      "Epoch 00043: val_acc improved from 0.71396 to 0.72203, saving model to C:/Users/athen/Documents/CDS 490/flower\\models\\model_ex-043_acc-0.722030.h5\n",
      "108/108 [==============================] - 9631s 89s/step - loss: 0.3198 - acc: 0.8805 - val_loss: 0.9121 - val_acc: 0.7220\n",
      "Epoch 44/100\n",
      "28/28 [==============================] - 163s 6s/step - loss: 0.9254 - acc: 0.7070\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.72203\n",
      "108/108 [==============================] - 9637s 89s/step - loss: 0.2795 - acc: 0.8964 - val_loss: 0.9254 - val_acc: 0.7070\n",
      "Epoch 45/100\n",
      "28/28 [==============================] - 165s 6s/step - loss: 0.9198 - acc: 0.7093\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.72203\n",
      "108/108 [==============================] - 9636s 89s/step - loss: 0.2861 - acc: 0.8981 - val_loss: 0.9198 - val_acc: 0.7093\n",
      "Epoch 46/100\n",
      "28/28 [==============================] - 169s 6s/step - loss: 0.9038 - acc: 0.7313\n",
      "\n",
      "Epoch 00046: val_acc improved from 0.72203 to 0.73126, saving model to C:/Users/athen/Documents/CDS 490/flower\\models\\model_ex-046_acc-0.731257.h5\n",
      "108/108 [==============================] - 10423s 97s/step - loss: 0.2746 - acc: 0.9054 - val_loss: 0.9038 - val_acc: 0.7313\n",
      "Epoch 47/100\n",
      "28/28 [==============================] - 168s 6s/step - loss: 1.0154 - acc: 0.6794\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.73126\n",
      "108/108 [==============================] - 10005s 93s/step - loss: 0.2617 - acc: 0.9060 - val_loss: 1.0154 - val_acc: 0.6794\n",
      "Epoch 48/100\n",
      "28/28 [==============================] - 166s 6s/step - loss: 0.9953 - acc: 0.7116\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.73126\n",
      "108/108 [==============================] - 10096s 93s/step - loss: 0.2630 - acc: 0.9025 - val_loss: 0.9953 - val_acc: 0.7116\n",
      "Epoch 49/100\n",
      "28/28 [==============================] - 167s 6s/step - loss: 0.9538 - acc: 0.7128\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.73126\n",
      "108/108 [==============================] - 10630s 98s/step - loss: 0.2450 - acc: 0.9144 - val_loss: 0.9538 - val_acc: 0.7128\n",
      "Epoch 50/100\n",
      "28/28 [==============================] - 166s 6s/step - loss: 0.9611 - acc: 0.7197\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.73126\n",
      "108/108 [==============================] - 9921s 92s/step - loss: 0.2341 - acc: 0.9167 - val_loss: 0.9611 - val_acc: 0.7197\n",
      "Epoch 51/100\n",
      "28/28 [==============================] - 165s 6s/step - loss: 1.0710 - acc: 0.7163\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.73126\n",
      "108/108 [==============================] - 9880s 91s/step - loss: 0.2355 - acc: 0.9091 - val_loss: 1.0710 - val_acc: 0.7163\n",
      "Epoch 52/100\n",
      "28/28 [==============================] - 170s 6s/step - loss: 0.9949 - acc: 0.7151\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.73126\n",
      "108/108 [==============================] - 9843s 91s/step - loss: 0.2374 - acc: 0.9138 - val_loss: 0.9949 - val_acc: 0.7151\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 164s 6s/step - loss: 0.9692 - acc: 0.7278\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.73126\n",
      "108/108 [==============================] - 9797s 91s/step - loss: 0.2169 - acc: 0.9219 - val_loss: 0.9692 - val_acc: 0.7278\n",
      "Epoch 54/100\n",
      "28/28 [==============================] - 165s 6s/step - loss: 1.1583 - acc: 0.7082\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.73126\n",
      "108/108 [==============================] - 9813s 91s/step - loss: 0.2248 - acc: 0.9164 - val_loss: 1.1583 - val_acc: 0.7082\n",
      "Epoch 55/100\n",
      "28/28 [==============================] - 171s 6s/step - loss: 1.0838 - acc: 0.6851\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.73126\n",
      "108/108 [==============================] - 10029s 93s/step - loss: 0.2093 - acc: 0.9271 - val_loss: 1.0838 - val_acc: 0.6851\n",
      "Epoch 56/100\n",
      "28/28 [==============================] - 185s 7s/step - loss: 1.0320 - acc: 0.7186\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.73126\n",
      "108/108 [==============================] - 10120s 94s/step - loss: 0.2027 - acc: 0.9268 - val_loss: 1.0320 - val_acc: 0.7186\n",
      "Epoch 57/100\n",
      "28/28 [==============================] - 165s 6s/step - loss: 1.2029 - acc: 0.6920\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.73126\n",
      "108/108 [==============================] - 9815s 91s/step - loss: 0.2174 - acc: 0.9222 - val_loss: 1.2029 - val_acc: 0.6920\n",
      "Epoch 58/100\n",
      "28/28 [==============================] - 166s 6s/step - loss: 1.0905 - acc: 0.7024\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.73126\n",
      "108/108 [==============================] - 9854s 91s/step - loss: 0.1881 - acc: 0.9340 - val_loss: 1.0905 - val_acc: 0.7024\n",
      "Epoch 59/100\n",
      "28/28 [==============================] - 166s 6s/step - loss: 1.0760 - acc: 0.7140\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.73126\n",
      "108/108 [==============================] - 9844s 91s/step - loss: 0.1941 - acc: 0.9274 - val_loss: 1.0760 - val_acc: 0.7140\n",
      "Epoch 60/100\n",
      "28/28 [==============================] - 169s 6s/step - loss: 1.0699 - acc: 0.7128\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.73126\n",
      "108/108 [==============================] - 9829s 91s/step - loss: 0.1846 - acc: 0.9274 - val_loss: 1.0699 - val_acc: 0.7128\n",
      "Epoch 61/100\n",
      "28/28 [==============================] - 165s 6s/step - loss: 1.0972 - acc: 0.7186\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.73126\n",
      "108/108 [==============================] - 9839s 91s/step - loss: 0.1826 - acc: 0.9337 - val_loss: 1.0972 - val_acc: 0.7186\n",
      "Epoch 62/100\n",
      "28/28 [==============================] - 167s 6s/step - loss: 1.0556 - acc: 0.7186\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.73126\n",
      "108/108 [==============================] - 9806s 91s/step - loss: 0.1551 - acc: 0.9468 - val_loss: 1.0556 - val_acc: 0.7186\n",
      "Epoch 63/100\n",
      "28/28 [==============================] - 165s 6s/step - loss: 1.1520 - acc: 0.7163\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.73126\n",
      "108/108 [==============================] - 9823s 91s/step - loss: 0.1649 - acc: 0.9410 - val_loss: 1.1520 - val_acc: 0.7163\n",
      "Epoch 64/100\n",
      "28/28 [==============================] - 174s 6s/step - loss: 1.2213 - acc: 0.7163\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.73126\n",
      "108/108 [==============================] - 10779s 100s/step - loss: 0.1444 - acc: 0.9462 - val_loss: 1.2213 - val_acc: 0.7163\n",
      "Epoch 65/100\n",
      "28/28 [==============================] - 149s 5s/step - loss: 1.1022 - acc: 0.7174\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.73126\n",
      "108/108 [==============================] - 9796s 91s/step - loss: 0.1361 - acc: 0.9537 - val_loss: 1.1022 - val_acc: 0.7174\n",
      "Epoch 66/100\n",
      "28/28 [==============================] - 133s 5s/step - loss: 1.1267 - acc: 0.7197\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.73126\n",
      "108/108 [==============================] - 7868s 73s/step - loss: 0.1425 - acc: 0.9488 - val_loss: 1.1267 - val_acc: 0.7197\n",
      "Epoch 67/100\n",
      "28/28 [==============================] - 131s 5s/step - loss: 1.1217 - acc: 0.7140\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.73126\n",
      "108/108 [==============================] - 7639s 71s/step - loss: 0.1440 - acc: 0.9479 - val_loss: 1.1217 - val_acc: 0.7140\n",
      "Epoch 68/100\n",
      "28/28 [==============================] - 132s 5s/step - loss: 1.1314 - acc: 0.7186\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.73126\n",
      "108/108 [==============================] - 7811s 72s/step - loss: 0.1316 - acc: 0.9514 - val_loss: 1.1314 - val_acc: 0.7186\n",
      "Epoch 69/100\n",
      "28/28 [==============================] - 132s 5s/step - loss: 1.3170 - acc: 0.7197\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.73126\n",
      "108/108 [==============================] - 7673s 71s/step - loss: 0.1405 - acc: 0.9462 - val_loss: 1.3170 - val_acc: 0.7197\n",
      "Epoch 70/100\n",
      "28/28 [==============================] - 131s 5s/step - loss: 1.1520 - acc: 0.7209\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.73126\n",
      "108/108 [==============================] - 7610s 70s/step - loss: 0.1423 - acc: 0.9482 - val_loss: 1.1520 - val_acc: 0.7209\n",
      "Epoch 71/100\n",
      "28/28 [==============================] - 131s 5s/step - loss: 1.1562 - acc: 0.7163\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.73126\n",
      "108/108 [==============================] - 7640s 71s/step - loss: 0.1377 - acc: 0.9505 - val_loss: 1.1562 - val_acc: 0.7163\n",
      "Epoch 72/100\n",
      "28/28 [==============================] - 132s 5s/step - loss: 1.1697 - acc: 0.7151\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.73126\n",
      "108/108 [==============================] - 7628s 71s/step - loss: 0.1419 - acc: 0.9436 - val_loss: 1.1697 - val_acc: 0.7151\n",
      "Epoch 73/100\n",
      "28/28 [==============================] - 134s 5s/step - loss: 1.2179 - acc: 0.7174\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.73126\n",
      "108/108 [==============================] - 7630s 71s/step - loss: 0.1477 - acc: 0.9442 - val_loss: 1.2179 - val_acc: 0.7174\n",
      "Epoch 74/100\n",
      "28/28 [==============================] - 131s 5s/step - loss: 1.2189 - acc: 0.7186\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.73126\n",
      "108/108 [==============================] - 7644s 71s/step - loss: 0.1416 - acc: 0.9540 - val_loss: 1.2189 - val_acc: 0.7186\n",
      "Epoch 75/100\n",
      " 61/108 [===============>..............] - ETA: 55:30 - loss: 0.1323 - acc: 0.9518"
     ]
    }
   ],
   "source": [
    "model_trainer.trainModel(num_objects=5, num_experiments=100, enhance_data=True, batch_size=32, show_network_summary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was able to get the above code running. It does take quite a long time to run. Each Epoch takes about 2 hours to run so the above code will take a few days. But I believe that is just the nature of image detection algorithms. The algorithm is working as the loss score continues to decrease and the acc. score has been increasing. I do believe this is the algorithm I will continue to use. \n",
    "\n",
    "My problem was the definition of num_objects was not what I thought it was. I thought it was the number of image types in your folder, based on the language they used in their definition, but it is the number of categories for your image detection, in my case the types of flowers. \n",
    "\n",
    "I do still need to work on the below code now that I have gotten the above code working.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # needed for the second part of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example code from ImageAI's github. Will need to change once I can get the above code working to fit my data. \n",
    "\n",
    "execution_path = os.getcwd()\n",
    "\n",
    "prediction = CustomImagePrediction()\n",
    "prediction.setModelTypeAsResNet()\n",
    "prediction.setModelPath(os.path.join(execution_path, \"idenprof_061-0.7933.h5\"))  # <- needs to change \n",
    "prediction.setJsonPath(os.path.join(execution_path, \"model_class.json\"))  # <- needs to change\n",
    "prediction.loadModel(num_objects=10)  # <- needs to change to 5 I believe\n",
    "\n",
    "predictions, probabilities = prediction.predictImage(os.path.join(execution_path, \"4.jpg\"), result_count=5)  # <- needs to change\n",
    "\n",
    "for eachPrediction, eachProbability in zip(predictions, probabilities):\n",
    "    print(eachPrediction + \" : \" + eachProbability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What other datasets can I use?\n",
    "Oxford University has two different datasets I can use. \n",
    "One has 17 different types of flowers, https://www.robots.ox.ac.uk/~vgg/data/flowers/17/index.html , \n",
    "and the other has 102 different types of flowers, https://www.robots.ox.ac.uk/~vgg/data/flowers/102/index.html.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What other methods might I use?\n",
    "I will likely look into useing Tensorflow instead of ImageAI, because ImageAI's code is not working at the moment. If I can't get the code to work then I will likely switch over. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How will I validate my results?\n",
    "I plan on validating my results by printing out the likelihood of the correct flower. I.e having the program (which is what ImageIA does) print out the likelihood of each flower being the correct match. \n",
    "\n",
    "'daisy'  91.982383 \n",
    "\n",
    "'sunflower'   7.2389283 \n",
    "\n",
    "'rose'    1.211233\n",
    "\n",
    "'tulip'   0.521122\n",
    "\n",
    "'dandelion'    .4321382\n",
    "\n",
    "\n",
    "And if the program is able to output the correct flower at a 90% rating or above then I will count that flower type's image recognition as being successful. If the flower percentage rating falls below 80% as the lowest then I will consider that flower's image recognition to be unsuccessful. If the rating is between 80% and 90% then I will consider that to be somewhat successful.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototypeing 2\n",
    "\n",
    "My hypothesis is that the algorithm will be able to detect a fair ammount of flowers in my database but I do think it might have some problems with the images that are not focused on the flowers and are more centered around other objects, like a rocking chair with the flower in the corner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
